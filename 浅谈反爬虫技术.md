# 浅谈反爬虫技术
## 引论
- 大数据时代的到来，人们开始意识到数据的重要性，无论是互联网行业还是传媒行业等等，数据都已经成为行业、企业中的重要资产。拿当今字节跳动旗下两款爆红的产品——“抖音”、“今日头条”来说，个性化推荐已经成为吸引受众和保持热度的一大关键因素，而个性化推荐的实现依靠的是核心算法以及大量用户数据所形成的喜好表现的支持，包括在电商行业、新闻行业，数据的收集的重要性已经不可替代，随着现在人工智能的发展与渐渐成熟，数据的深度挖掘更是不可小觑。在互联网时代，网络爬虫是一个很普遍的技术，很多人做项目、调查、创业、写论文等工作时需要一些数据的支撑，就会用网络爬虫去特定的网站抓取数据，并且网络爬虫的入门门槛低，下载一个开源的爬虫就可搞定，这就造成网络上爬虫横行，轻则消耗网络和机器
资源，影响网站服务的访问速度，重则网站无法访问，甚至窃取宝贵的核心数据，尤其是在数据时代，数据经常是一个公司的核心竞争力，如何反爬虫变成了一个长期的斗争。所以这篇文章针对常见的数据收集手段——爬虫，做以简单介绍，简要说明常见的爬虫手段，主要介绍常见的爬虫攻防手段、反爬虫策略，以及爬虫应用的领域。

## 关键字：
- 爬虫，反爬虫，反爬虫技术，反爬虫策略

### 一、爬虫工作原理简介
- 要想知道如何反爬虫，我们先来了解下目前网络上常见的网络爬虫是如何工作的。大约分为如下几个步骤：
    - 分析对方网站的URL路径；
    - 了解对方网站页面数据请求和返回的格式；
    - 解析目标字段；
    - 创建合适的http 请求，用正常的User-Agent，让服务器误以为你是浏览器在打开网页，不是机器抓取数据；
    - 批量发送http 请求，获取数据；
    - 代理IP 池，每次轮换不同的IP 去请求数据；
    - 遇到登陆界面的，给一个Cookies；
    - 降低请求频率，批量抓取数据，防止IP被封。
- 到底什么是网络爬虫呢？我们常见的搜索引擎（例如百度）、抢票软件，其实充分利用了爬虫技术，搜索引擎每天都要发出不计其数的爬虫到各个大大小小的网站，收集相关信心，最后内部整理消化，呈现给用户，抢票软件也放出无数个爬虫来不断刷新火车票售票网站的余票，一旦有，便可告知用户付款，这都是爬虫应用的最好的举例。更多的例子还包括：伯乐在线、惠惠购物助手、数据分析与研究（数据冰山知乎专栏）等。

- 所谓爬虫，如果从技术原理上讲，它就是一个高效的下载工具，能够批量将网页下载到本地，留作备份。如果结合一些其他工具和算法，就能够实现，收集同一类型的网页，重复执行同一动作等行为。简单讲，就是通过技术和算法模拟一个人在网络上的行为，像人一样点网页，像人一样下订单，只不过，相比起真人，他的效率高的异常。
- 网络爬虫(Web crawler)，就是通过网址获得网络中的数据、然后根据目标解析数据、存储目标信息。这个过程可以自动化程序实现，行为类似一个蜘蛛。蜘蛛在互联网上爬行，一个一个网页就是蜘蛛网。这样蜘蛛可以通过一个网页爬行到另外一个网页。
- 通用爬虫：通用爬虫是搜索引擎抓取系统（百度、谷歌、搜狗等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。
- 聚焦爬虫：是面向特定需求的一种网络爬虫程序，他与通用爬虫的区别在于：聚焦爬虫在实施网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。
- 简单来说，爬虫就是获取网页并提取和保存信息的自动化程序，下面概要介绍一下。

    - **获取网页**
        - 爬虫首先要做的工作就是获取网页，这里就是获取网页的源代码。源代码里包含了网页的部分有用信息，源代码一旦获得，我们就可以从中提取想要的信息了。
        - 获取网页数据，也就是通过网址（ URL：Uniform Resource Locator,统一资源 定位符），获得网络的数据，充当搜索引擎。当输入网址，我们就相当于对网址服务器发送了一个请求，网站服务器收到以后，进行处理和解析，进而给我们一个相应的相应。如果网络正确并且网址不错，一般都可以得到网页信息，否则告诉我们一个错误代码，比如404. 整个过程可以称为请求和响应。在学会爬虫之前，我们首先要理解请求和响应的概念，向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来，这个流程的实现就很关键，我们不会依靠手工去截取源代码，这样不符合我们的效率，不符合我们的实际操作情况。常见的请求方法有两种，GET和 POST。GET请求是把参数包含在了url里面，比如在百度里面输入爬虫，得到一个get 请求，链接为 https://www.baidu.com/s?wd=爬虫。而post请求大多是在表单里面进行，也就是让你输入用户名和秘密，在url里面没有体现出来，这样更加安全。post请求的大小没有限制，而get请求有限制，最多1024个字节。这个时候，Python的好处就完全体现出来，Python提供了许多库来帮助我们实现这个操作，如urllib、requests等。我们可以用这些库来帮助我们实现HTTP请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的Body部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。
        - 在python程序里面，上述过程可以通过获取网页中的源代码实现，进而获得网页中的数据。首先看一下网址的源代码查看方法，使用google浏览器，右键选择检查，查看需要爬取的网址源代码，具体如下：从图可得知，在Network选项卡里面，点击第一个条目，也就是http://www.baidu.com，看到源代码。
    - **提取信息**
    
        - 获取网页源代码后，接下来就是分析网页源代码，从中提取我们想要的数据。在第一步，我们获得了网页的源代码，也就是数据。然后就是解析里面的数据，为我们的分析使用。常见的方法有很多，比如正则表达式、xpath解析等。最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。
        - 在Python语言中，我们经常使用Beautiful Soup、pyquery、lxml等库，可以高效的从中获取网页信息，如节点的属性、文本值等。Beautiful Soup库是解析、遍历、维护“标签树”的功能库，对应一个HTML/XML文档的全部内容。
        - 另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS选择器或XPath来提取网页信息的库，如Beautiful Soup、pyquery、lxml等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。

    - **存储网页数据**
        - 提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为TXT文本或JSON文本，也可以保存到数据库，如MySQL和MongoDB等，也可保存至远程服务器，如借助SFTP进行操作等。
        - 解析完数据以后，就可以保存起来。如果不是很多，可以考虑保存在txt 文本、csv文本或者json文本等，如果爬取的数据条数较多，我们可以考虑将其存储到数据库中。因此，我们需要学会 MySql、MongoDB、SqlLite的用法。更加深入的，可以学习数据库的查询优化。

        - JSON(JavaScript Object Notation) 是一种轻量级的数据交换格式。它基于ECMAScript的一个子集。 JSON采用完全独立于语言的文本格式，但是也使用了类似于C语言家族的习惯(包括C、C++、Java、JavaScript、Perl、Python等)。这些特性使JSON成为理想的数据交换语言。易于人阅读和编写，同时也易于机器解析和生成(一般用于提升网络传输速率)。JSON在python中分别由list和dict组成。Python官方json网址是 https://docs.python.org/3/library/json.html?highlight=json#module-json，具体使用方法如下：

    ```
    with open('douban_movie_250.csv','a',encoding='utf-8') as f:
        f.write(json.dumps(content,ensure_ascii=False)+'\n')
    ```

    保存数据是比较简单且最基本的操作，这里我不再赘述。

    - **自动化程序**
        - 自动化程序的含义是说，爬虫取代人工更加高效的完成这些操作。我们不能否认手工无法完成这些信息，但是不排除一些特殊情况，比如我们立即需要这些数据，即刻需要用到，或者量非常的大，人力速度过慢且无法胜任的情况下，我们必须要借助程序。爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行各种异常处理、错误重试等操作，确保爬取持续高效地运行。那么爬虫到底可以爬取哪些类型的数据呢？
        - 在网页中我们能看到各种各样的信息，最常见的便是常规网页，它们对应着HTML代码，而最常抓取的便是HTML源代码。另外，可能有些网页返回的不是HTML代码，而是一个JSON字符串（其中API接口大多采用这样的形式），这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。此外，我们还可以看到各种二进制数据，如图片、视频和音频等。利用爬虫，我们可以将这些二进制数据抓取下来，然后保存成对应的文件名。而且还可以看到各种扩展名的文件，如CSS、JavaScript和配置文件等，这些其实也是最普通的文件，只要在浏览器里面可以访问到，就可以将其抓取下来。上述内容其实都对应各自的URL，是基于HTTP或HTTPS协议的，只要是这种数据，爬虫都可以抓取。
        - 爬虫的目的是分析网页数据，进的得到我们想要的结论。在 python数据分析中，我们可以使用使用第三步保存的数据直接分析，主要使用的库如下：NumPy、Pandas、 Matplotlib 三个库。NumPy ：它是高性能科学计算和数据分析的基础包。Pandas : 基于 NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。它可以算得上作弊工具。Matplotlib：Python中最著名的绘图系统Python中最著名的绘图系统。它可以制作出散点图，折线图，条形图，直方图，饼状图，箱形图散点图，折线图，条形图，直方图，饼状图，箱形图等。
        
### 二、爬虫实现方法简介
- 了解了爬虫的工作原理，那么爬虫访问网站，抓取
数据时常用的方法有：
    - **伪装User-Agent**：服务器通过User-Agent字段
知道访问网站的谁，每个浏览器都有正规的固定的
User-Agent，爬虫只要伪装成正规的浏览器，服务器是
分辨不出来的。
    - **基于IP 代理**：使用IP代理轮流访问网站，使访问服务器的时间延迟加大，频率减小，服务器难以检测。
    - **基于Cookies**：研究目标网站的Cookies过期事件，可以模拟浏览器，定时生成Cookies 访问网站而不被封
    - **限速访问**：如果爬虫循环无休眠地暴力爬取数据，那IP 随时被封，爬虫的限速访问实现容易，抓取时间长，效率也高，结合IP 代理可以很快地实现爬取目标内容
- java实现：
1. **基于socket通信编写爬虫**：这最底层的方式，同时也是执行最高效的，不过开发效率最低。
2. **基于HttpURLConnection类编写爬虫**：java se的net包的核心类，主要用于http的相关操作。
3. **基于apache的HttpClient包编写爬虫**：由net包拓展而来，专为java网络通信编程而服务。
4. **基于phantomjs之类的无头（无界面）浏览器**：
    - 它是浏览器的核心，并非浏览器。换言之，它是没有UI的浏览器。
    - 它提供的js api，故它可以方便直接的被各种程序语言调用。换言之，似乎是js写的。
5. **基于Selenium或者是WebDriver之类的有头（有界面）浏览器**
    -它是浏览器核心，并非浏览器。换言之，它是没有界面UI的浏览器。无头，即无界面。
    -它提供的js api,故它可以方便直接的被各种程序语言调用。
- 在本文中，主要针对的是反爬虫技术，所以对于一般常见的爬虫技术不再一一详解。


### 三、常见反爬虫技术
- 从网络开始的那一刻起，爬虫就肩负了她的使命，数据收集！尤其是大数据时代的到来，越来越多的企业认识到数据的重要性，数据成了一个企业的重要资产，数据的多样性给了爬虫更高的使命。
- 爬虫是模拟人的浏览访问行为，进行数据的批量抓取。当抓取数据量逐渐增大时，会被访问的服务器造成很大的压力，甚至有可能会崩溃。换句话说，服务器不喜欢有人抓取自己的数据，那么网站方面就会针对这些爬虫者，采取一些反爬策略。服务器第一种识别爬虫的方式就是通过检查连接的useragent来识别到底是浏览器访问，还是代码访问。如果是代码访问的话，访问量增大时，服务器就会直接封掉来访IP。那么应对这种初级的反爬机制，我们应该采取何种措施？针对上面的爬虫常用的方法，先概括谈一下反爬虫的技术从哪些方面考虑：
    - **IP 限制**：在后台对访问进行统计，设定单个IP访问的阈值，如果一个IP 地址在短时间内访问频率超过阈值，可以暂时对这个IP 予以封锁。可以用netstat检查80 端口的连接，该命令对80 端口连接数量的来源IP进行排序，直观的判断出网络爬虫，一般来说爬虫的并发连接是非常高的。拒绝网络爬虫的请求可以通过内核防火墙来拒绝，也可以在服务器端进行设置来拒绝。这个虽然效果还不错，但是其实有两个缺陷，一个是非常容易误伤普通用户，另一个就是IP 其实不值钱，网张上有大量的代理IP 出售。

    ```
    netstat -nt | grep hostip:80 | awk '{print $5}' | awk -F":" '{print $1}'|
    sort | uniq -c | sort -r -n
    ```

    - **基于Headers**：Headers是区分浏览器行为和机器行为最简单的方法。早期的爬虫经常会随便写个User-Agent，或是省略了referer，这种很好检测。现在的爬虫它们开始模拟不同的User-Agent和referer。
    - **基于爬虫的User-Agent信息**：很多爬虫并不会以很高的并发连接爬取，或者爬虫的代理IP分布很广，很难通过封锁IP地址来解决问题。这种情况下我们可以通过爬虫的User-Agent信息来识别。每个爬虫在爬取网页的时候会声明自己的User-Agent信息，我们可以在日志中记录每个请求的User-Agent信息，从中找出访问量最大的那些User-Agent，从中可以看出每个爬虫的请求次数，根据User-Agent信息来封锁爬虫。
    - **基于网站流量统计和日志分析**：在用socket、httpurlconnection、httpclient 等写的爬虫，是建立在抓包分析后，得到请求的关键URL请求流基础之上的，会造成很多的请求是不在请求范围中的，这些不在范围中的请求，多跟数据获取无关，但对服务器确实有很大的影响的，尤其是富浏览器端如此盛行的当下，如一些ajax 请求、广告请求、正常的相关请求资源连接等，都会被分析给过滤掉。这种情况下我们可以通过网站流量系统记录的真实用户访问IP 来进行识别。在服务器端对访问记录做一些相关的统计追踪，可以很容易发现哪些IP 下一定是有爬虫存在，而非正常用户的浏览器操作行为，从而达到检查到反爬、实施反爬行为的目的。
    - 网站流量统计主要有两种方法：一种方法是直接分析服务器日志，统计网站访问量。另一种方法是在网页中嵌入一段javascript代码，让服务器程序响应客户段js 的请求，分析和识别请求，然后写日志的同时做后台的异步统计了。通过流量统计系统得到的用户IP基本是真实的用户访问，因为一般情况下爬虫是无法执行网页里面的js代码片段的。所以我们可以拿流量统计系统记录的IP 和服务器程序日志记录的IP地址进行比较，如果服务器日志里面某个IP 发起了大量的请求，在流量统计系统里面却根本找不到，或者即使找得到，可访问量却只有寥寥几个，那么无疑就是一个网络爬虫。在对比结果中，排除真实用户访问IP，再排除我们希望放行的网页爬虫，如百度等搜索引擎爬虫等，最后就是爬虫的IP地址了，然后通知管理员进行相应的处理。
    - **基于访问的来源username**：像新浪微博类，需要登陆后方可访问到所需要的资源，这样可以在服务器的后台设置相关的阈值，如某用户某时间段请求的页面数，是否存在使用代理服务器的情况，是否有异地登陆等情况的出现，当有异常情况出现时，配合手动输入的验证码，可以拒绝一部分的爬虫。
- 几种反爬技术结合的反爬策略：反爬虫技术要尽量不要误伤正常使用的用户，实现的方式就是多种方法结合在一起，比如IP+UserAgent，Username+User-Agent、IP+UserName、或者是IP、Username 加一些cookies 中参数的组合，如cookie_id等，这样可以减少被反爬影响的主机和用户范围。
 
- 在进行访问时，我们在开发者环境下不仅可以找到URL、Form Data、还可以在request headers中构造浏览器的请求头，封装自己。服务器识别浏览器的访问方式就是判断keywor是否为Request headers 下的User-Agent。我们只需要构造这个请求头的参数。创建请求头部信息即可，代码如下：
```
    headers = {'User-Agent':    'Mozilla/5.0    (Windows    NT    6.1;    WOW64)'AppleWebKit/537.36 (KHTML,like Gecko) Chrome/43.0.2357.124 Safari/537.36'}

    response =requests.get(url,headers=headers) 
```
- 很多人会认为修改User-Agent太简单，确实很简单，但正常人一秒看一个图，而爬虫一秒可以看几百张图，那么服务器的压力必然增大。就是说，如果在一个IP下批量访问下载图片，这个行为不符合正常人类的行为，肯定要被封IP。其原理也很简单，就是统计每个IP的访问频率，此频率超过阀值，就会返回一个验证码，如果真的是用户访问的话，用户就会填写，然后继续访问，如果是代码访问的话，就会被封IP。这个问题的解决方法有两个，第一个就是常用的增设延时，每三秒抓取一次，代码如下：

   ``` 
   import time
   time.sleep(3)
   ```
- 其实，还有一个更重要的方法，那就是从本质解决问题。
不管如何访问，服务器的目的就是查出哪些为代码访问，然后封锁IP。解决办法：为避免被封锁IP，在数据采集之前经常会使用代理。当然requests也有相应的proxies属性。首先构建自己的代理IP池，将其以字典的形式赋值给proxies，然后传输给requests，代码如下：
```
    proxies = ｛

        "http": "http://10.10.1.10:3128",

        "https": "http://10.10.1.10.1080",

｝

    response = requests.get(url, proxies=proxies)
```
- 从功能上来讲，爬虫一般分为数据采集，处理，储存三个部分。这里我们只讨论数据采集部分。一般网站从三个方面反爬虫：用户请求的Headers，用户行为，网站目录和数据加载方式。前两种比较容易遇到，大多数网站都从这些角度来反爬虫。第三种一些应用ajax的网站会采用，这样增大了爬取的难度。
1. **通过Headers反爬虫**
- 从用户请求的Headers反爬虫是最常见的反爬虫策略。很多网站都会对Headers的User-Agent进行检测，还有一部分网站会对Referer进行检测（一些资源网站的防盗链就是检测Referer）。如果遇到了这类反爬虫机制，可以直接在爬虫中添加Headers，将浏览器的User-Agent复制到爬虫的Headers中；或者将Referer值修改为目标网站域名。对于检测Headers的反爬虫，在爬虫中修改或者添加Headers就能很好的绕过。
2. **基于用户行为反爬虫**
- 还有一部分网站是通过检测用户行为，例如同一IP短时间内多次访问同一页面，或者同一账户短时间内多次进行相同操作。
大多数网站都是前一种情况，对于这种情况，使用IP代理就可以解决。可以专门写一个爬虫，爬取网上公开的代理ip，检测后全部保存起来。这样的代理ip爬虫经常会用到，最好自己准备一个。有了大量代理ip后可以每请求几次更换一个ip，这在requests或者urllib2中很容易做到，这样就能很容易的绕过第一种反爬虫。
- 对于第二种情况，可以在每次请求后随机间隔几秒再进行下一次请求。有些有逻辑漏洞的网站，可以通过请求几次，退出登录，重新登录，继续请求来绕过同一账号短时间内不能多次进行相同请求的限制。
3. **动态页面的反爬虫**
- 上述的几种情况大多都是出现在静态页面，还有一部分网站，我们需要爬取的数据是通过ajax请求得到，或者通过JavaScript生成的。首先用Firebug或者HttpFox对网络请求进行分析。如果能够找到ajax请求，也能分析出具体的参数和响应的具体含义，我们就能采用上面的方法，直接利用requests或者urllib2模拟ajax请求，对响应的json进行分析得到需要的数据。
- 能够直接模拟ajax请求获取数据固然是极好的，但是有些网站把ajax请求的所有参数全部加密了。我们根本没办法构造自己所需要的数据的请求。我这几天爬的那个网站就是这样，除了加密ajax参数，它还把一些基本的功能都封装了，全部都是在调用自己的接口，而接口参数都是加密的。遇到这样的网站，我们就不能用上面的方法了，我用的是selenium+phantomJS框架，调用浏览器内核，并利用phantomJS执行js来模拟人为操作以及触发页面中的js脚本。从填写表单到点击按钮再到滚动页面，全部都可以模拟，不考虑具体的请求和响应过程，只是完完整整的把人浏览页面获取数据的过程模拟一遍。
- 用这套框架几乎能绕过大多数的反爬虫，因为它不是在伪装成浏览器来获取数据（上述的通过添加 Headers一定程度上就是为了伪装成浏览器），它本身就是浏览器，phantomJS就是一个没有界面的浏览器，只是操控这个浏览器的不是人。利用 selenium+phantomJS能干很多事情，例如识别点触式（12306）或者滑动式的验证码，对页面表单进行暴力破解等等。它在自动化渗透中还 会大展身手，以后还会提到这个。

### 四、反爬虫的应用
- 据有关数据显示，爬虫流量目标行业的分布，集中在新闻领域、搜索领域、地图、自媒体、运营商、政府部门、电商、出行以及社交领域等等。其中社交和出行占据了较大部分的比例，而最终排名第一的是出行。
- 根据 Aberdeen Group 在近期发布的以北美几百家公司数据为样本的爬虫调查报告显示，2015 年网站流量中的真人访问仅为总流量的 54.4% ，剩余的流量由 27% 的好爬虫和 18.6% 的恶意爬虫构成。
- 恶意爬虫占比数据与 2013 年和 2014 年相比有所下降，同时真人访问的占比也有所提升，但这并不意味着恶意爬虫日渐式微。一个原因是印度、印度尼西亚等高人口总数国家的互联网新增人口有大幅提升，另一方面，恶意爬虫制造者更专注于爬虫的质量而不是数量，如今的恶意爬虫具有高持续性和可变性。

- 爬与反爬的斗争从未间断。过去的初级爬虫能很明显从异常的 Headers 信息甄别，但爬虫制造者从一次次爬与反爬中总结出可能被封的原因，通过不断的测试和改善爬虫程序，更新换代后的高持续性恶意爬虫通常具有以下特点中的某几个：
    - 模仿真人行为
    - 加载 Javascript 和外部资源
    - 模拟 cookie 和 useragent
    - 浏览器自动化操作
    - 变化的 IP 地址池
- 可能很多人认为，恶意爬虫只会威胁到少数以文本为核心价值的网站，其实这些能改变自己请求路径和请求方式的伪装者可能潜伏在任何一个网站的每一个角落，文本、图片、价格、评论、接口、架构等方方面面均有可能成为爬虫的囊中物。


## 结语：
- 总之在信息时代，我们综合运用上面的各种反爬虫方法，可以很大程度上缓解爬虫对网站造成的负面影响，保证网站的正常访问。但是，只要有利益的地方就有江湖，爬虫、反爬虫、甚至反反爬虫之间的斗争不会停止，但无论怎样成熟、完美的反爬虫技术都能被攻破，唯一不能被攻破的是人工干预的反爬虫技术———编写的代码永远不能与人类的智慧相抗衡。



 

## 参考文献：
- [https://www.zhihu.com/question/24098641](https://www.zhihu.com/question/24098641)
- 《大数据时代的反爬虫技术》陈利婷
